---
title: "Homework 5"
author: "PSTAT 131"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Homework 5

For this assignment, we will be working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Vulpix, a Fire-type fox Pokémon from Generation 1 (also my favorite Pokémon!)](images/vulpix.png){width="196"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics. *This is an example of a **classification problem**, but these models can also be used for **regression problems***.

Read in the file and familiarize yourself with the variables using `pokemon_codebook.txt`.

```{r}
library(ggplot2)
library(ggthemes)
library(tidymodels)
library(tidyverse)

tidymodels_prefer()
pokemon<-read_csv("data/pokemon.csv")
```


### Exercise 1

Install and load the `janitor` package. Use its `clean_names()` function on the Pokémon data, and save the results to work with for the rest of the assignment. What happened to the data? Why do you think `clean_names()` is useful?

```{r}
library(janitor)
pokemon <- clean_names(pokemon)
```


### Exercise 2

Using the entire data set, create a bar chart of the outcome variable, `type_1`.

```{r}
pokemon %>%
  ggplot(aes(x = type_1)) +
  geom_bar()
```

How many classes of the outcome are there? Are there any Pokémon types with very few Pokémon? If so, which ones?

There's 18 classes of outcomes, Flying Pokemon is the fewest class which is significant fewer than other. The second rare is Fairy and the third is Ice.

For this assignment, we'll handle the rarer classes by grouping them, or "lumping them," together into an 'other' category. [Using the `forcats` package](https://forcats.tidyverse.org/), determine how to do this, and **lump all the other levels together except for the top 6 most frequent** (which are Bug, Fire, Grass, Normal, Water, and Psychic).

Convert `type_1`, `legendary`, and `generation` to factors.

```{r}
# convert to factors
pokemon$type_1 <- as.factor(pokemon$type_1)
pokemon$legendary <- as.factor(pokemon$legendary)

#lump up the other levels together except for the top 6
pokemon$type_1 <- pokemon$type_1 %>% fct_lump_n(6)
summary(pokemon)
```

### Exercise 3

Perform an initial split of the data. Stratify by the outcome variable. You can choose a proportion to use. Verify that your training and test sets have the desired number of observations.

Next, use *v*-fold cross-validation on the training set. Use 5 folds. Stratify the folds by `type_1` as well. *Hint: Look for a `strata` argument.*

```{r}
pokemon_split <- initial_split(pokemon, strata = type_1, prop = 0.75)

pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
```


Why do you think doing stratified sampling for cross-validation is useful?

Stratified sampling will make sure that type_1 is evenly distributed in each fold to avoid bias in fitting.

### Exercise 4

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the categorical variables for this plot; justify your decision(s).*

```{r}
library(corrplot)

pokemon_train %>% select(is.numeric) %>% cor() %>%
corrplot(type = 'lower',
         method = 'color')
```

What relationships, if any, do you notice?

The number has a very high correlation with generation, this make sense because new generation pokemon have higher number. And total has high correlation with all the predictors except generation, this is because a higher in individual ability must reflect as a higher in total.

### Exercise 5

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`.

-   Dummy-code `legendary` and `generation`;

-   Center and scale all predictors.

```{r}
# set generation as factor
pokemon_train$generation <- as.factor(pokemon_train$generation)
pokemon_test$generation <- as.factor(pokemon_test$generation)

# setup folds with a number of 5
pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata = type_1)

# setting up recipe
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pokemon_train) %>%
  step_dummy(legendary) %>%
  step_dummy(generation) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors(), -all_nominal()) %>%
  prep()

```


### Exercise 6

We'll be fitting and tuning an elastic net, tuning `penalty` and `mixture` (use `multinom_reg()` with the `glmnet` engine).

Set up this model and workflow. Create a regular grid for `penalty` and `mixture` with 10 levels each; `mixture` should range from 0 to 1. For this assignment, let `penalty` range from 0.01 to 3 (this is on the `identity_trans()` scale; note that you'll need to specify these values in base 10 otherwise).

```{r}
en <- multinom_reg(mixture = tune(), 
                   penalty = tune()) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

en_workflow <- workflow() %>% 
  add_recipe(pokemon_recipe) %>% 
  add_model(en)

en_grid <- grid_regular(
  penalty(range = c(0.01, 3),
          trans = identity_trans()),
  mixture(range = c(0, 1)),
  levels = 10)
```


### Exercise 7

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`; we'll be tuning `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why neither of those values would make sense.**

```{r}
rf_class_spec <- rand_forest(mtry = tune(), 
                             trees = tune(), 
                             min_n = tune()) %>%

  set_engine("ranger",importance = "impurity") %>% 
  set_mode("classification")

rf_class_wf <- workflow() %>% 
  add_model(rf_class_spec) %>% 
  add_recipe(pokemon_recipe)

rf_grid <- grid_regular(mtry(range = c(1, 8)), 
                        trees(range = c(5, 20)),
                        min_n(range = c(0, 10)),
                        levels = 8)
rf_grid
```

mtry means the number of parameters being considered at each split of the tree. min_n means the min number of observations at the terminal of the trees. mtry should in the range [min_n,max_n] to make sense cause you cannot exceed the range of the number of branches.

What type of model does `mtry = 8` represent?

It represents a bagging model.

### Exercise 8

Fit all models to your folded data using `tune_grid()`.

**Note: Tuning your random forest model will take a few minutes to run, anywhere from 5 minutes to 15 minutes and up. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit. We'll go over how to do this in lecture.**

```{r}
# Fit Elastic Net
en_results <- tune_grid(
  en_workflow,
  resamples = pokemon_folds,
  grid = en_grid
)

# Fit Random Forest
rf_results <- tune_grid(
  rf_class_wf,
  resamples = pokemon_folds,
  grid = rf_grid
)
```


Use `autoplot()` on the results. What do you notice? Do larger or smaller values of `penalty` and `mixture` produce better ROC AUC? What about values of `min_n`, `trees`, and `mtry`?

```{r}
# Visualize Elastic Net Results
autoplot(en_results, metric = "roc_auc") 

# Visualize Random Forest Results
autoplot(rf_results, metric = "roc_auc")
```

The result shows that a smaller value of Lasso Penalty tends to have greater roc_auc, and the roc_auc of random forest is increase with increase of number of trees and minimal node doesn't make that great contribution on the result. This make sense since smaller values for the Lasso penalty parameter would make the model more flexible, potentially capturing more patterns in the data. This could lead to a better classification performance as measured by ROC AUC. For the random forest, The plot shows that once we have sufficiently large minimal node sizes, the ROC AUC stabilizes.

What elastic net model and what random forest model perform the best on your folded data? (What specific values of the hyperparameters resulted in the optimal ROC AUC?)

```{r}
# Best Elastic Net Model
show_best(en_results, metric = "roc_auc")

# Best Random Forest Model
show_best(rf_results, metric = "roc_auc")
```

The model with penalty=0.01 mixture=0.222 and n=5 did best on the elastic net with a mean of 0.6720. The model with 20 trees min_n=0 mtry=4 did best in random forest with a mean of 0.7261.

### Exercise 9

Select your optimal [**random forest model**]{.underline}in terms of `roc_auc`. Then fit that model to your training set and evaluate its performance on the testing set.

Using the **training** set:

-   Create a variable importance plot, using `vip()`. *Note that you'll still need to have set `importance = "impurity"` when fitting the model to your entire training set in order for this to work.*

    -   What variables were most useful? Which were least useful? Are these results what you expected, or not?

Using the testing set:

-   Create plots of the different ROC curves, one per level of the outcome variable;

-   Make a heat map of the confusion matrix.

```{r}
library(vip)
best_rf_params <- show_best(rf_results, metric = "roc_auc", n = 1)
best_rf_spec <- rand_forest(
                    mtry = best_rf_params$mtry,
                    trees = best_rf_params$trees,
                    min_n = best_rf_params$min_n) %>%
                    set_engine("ranger",importance = "impurity") %>%
                    set_mode("classification")

best_rf_workflow <- workflow() %>% 
                    add_recipe(pokemon_recipe) %>% 
                    add_model(best_rf_spec)

best_rf_fit <- best_rf_workflow %>% 
               fit(data = pokemon_train)

best_rf_model <- pull_workflow_fit(best_rf_fit)
vip(best_rf_model)
```

Create Plots of ROC Curves.


```{r}
# Predict the class labels
predicted_labels <- predict(best_rf_fit, new_data = pokemon_test) %>% pull()

# Predict the probabilities
predicted_probs <- predict(best_rf_fit, new_data = pokemon_test, type = "prob") %>% pull()
```


```{r}

```

```{r}
library(caret)
conf_mat <- confusionMatrix(predicted_labels, pokemon_test$type_1)

# Create heatmap
# Convert the confusion matrix to tidy format
conf_mat_df <- as.table(as.matrix(conf_mat$table)) %>% as.data.frame()

# Create heatmap using ggplot2
ggplot(data = conf_mat_df, aes(x = Reference, y = Prediction)) +
  geom_tile(aes(fill = Freq), color = "white") +
  geom_text(aes(label = sprintf("%d", Freq)), vjust = 1) +
  scale_fill_gradient(low = "white", high = "blue") +
  theme_minimal() +
  labs(fill = "Frequency",
       title = "Confusion Matrix Heatmap",
       subtitle = "Reference vs Prediction")

```


### Exercise 10

How did your best random forest model do on the testing set?

Which Pokemon types is the model best at predicting, and which is it worst at? (Do you have any ideas why this might be?)

The pokemon with type others is the best at predicting with the highest sensitive and the model that worst predict at is Fire, Grass and Water. This makes sense since 'others' includes too many different types of individuals and therefore encompasses too many characteristics. As a result, unless a particular class has features that are especially distinct from the others, a Pokémon is easily categorized into 'others'. Because the above reason, the model is hard to classify Fire, Water, Grass as so first generation fundamental Pokemon from others so the model make a bad prediction on these class.
